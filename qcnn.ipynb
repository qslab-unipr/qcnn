{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Quantum Convolutional Neural Networks\n",
    "\n",
    "We implement a QCNN trained to classify the digits of the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import inspect\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates.embeddings import AmplitudeEmbedding, AngleEmbedding\n",
    "from pennylane import numpy as np\n",
    "import autograd.numpy as anp\n",
    "\n",
    "n_qubit = 8 #number of qubit in the circuit\n",
    "encoding = 'amplitude' #choose the quantum encoding: 'amplitude' or 'angle'\n",
    "num_classes = 10 # choose how many classes: 4, 6, 8, 10\n",
    "all_samples = True #True if you want all the samples, False, if you want only 250 samples for each class\n",
    "seed = 43 #set to None to generate the seed randomly\n",
    "U_params = 15 #number of parameters of F_2 circuit\n",
    "num_layer = 1 #number of convolutional layer repetitions\n",
    "load_params = False #if True load parameters from a file\n",
    "opt = 'Adam' #choose the optimizer: Adam, QNGO, or GDO\n",
    "lr = 0.01 #learning rate\n",
    "epochs = 2 #number of epochs\n",
    "batch_size = 64 #size of batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset: split it in train and test, take only 250 samples if all_samples == False.\n",
    "\n",
    "Take only 256 features if the amplitude encoding is applied, otherwise only 8 if the angle encoding is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of subset training data: (60000, 28, 28, 1)\n",
      "Shape of subset training labels: (60000,)\n",
      "Shape of subset training data: (60000, 28, 28, 1)\n",
      "Shape of subset training labels: (60000,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "It loads the MNIST dataset and then it processes the dataset based on the encoding method, number of classes and if we want all the samples. \n",
    "param encoding: indicate the quantum encoding used: 'amplitude' or 'angle'\n",
    "param num_classes: number of classes to be predicted, which samples take from the dataset\n",
    "param all_samples: True if we want all the samples, False to take only 250 samples for each class\n",
    "param seed: random_state seed\n",
    "return X_train, X_test, Y_train, Y_test: the dataset divided in training and test set\n",
    "\"\"\"\n",
    "def data_load_and_process(encoding, num_classes, all_samples, seed):\n",
    "\tif seed != None:\n",
    "\t\ttf.random.set_seed(seed)\n",
    "\t\tnp.random.seed(seed)\n",
    "\n",
    "\t(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "\tx_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\t # normalize the data\n",
    "\n",
    "\t#check is the user want all the samples\n",
    "\tif not all_samples:\n",
    "\t\tnum_examples_per_class = 250\n",
    "\t\tselected_indices = []\n",
    "\n",
    "\t\t# Iterate through each class to select 1000 examples\n",
    "\t\tfor class_label in range(10): \n",
    "\t\t\tindices = np.where(y_train == class_label)[0][:num_examples_per_class]\n",
    "\t\t\tselected_indices.extend(indices)\n",
    "\n",
    "\t\t# Filter the training data to contain only the selected examples\n",
    "\t\tx_train_subset = x_train[selected_indices]\n",
    "\t\ty_train_subset = y_train[selected_indices]\n",
    "\n",
    "\t\t# Shuffle the data\n",
    "\t\tshuffle_indices = np.random.permutation(len(x_train_subset))\n",
    "\t\tx_train = x_train_subset[shuffle_indices]\n",
    "\t\ty_train = y_train_subset[shuffle_indices]\n",
    "\n",
    "\tprint(\"Shape of subset training data:\", x_train.shape)\n",
    "\tprint(\"Shape of subset training labels:\", y_train.shape)\n",
    "\n",
    "\t#take only the number of classes selected\n",
    "\tmask_train = np.isin(y_train, range(0, num_classes))\n",
    "\tmask_test = np.isin(y_test, range(0, num_classes))\n",
    "\n",
    "\tX_train = x_train[mask_train]\n",
    "\tX_test = x_test[mask_test]\t\t\n",
    "\tY_train = y_train[mask_train]\n",
    "\tY_test = y_test[mask_test]\n",
    "\n",
    "\tprint(\"Shape of subset training data:\", X_train.shape)\n",
    "\tprint(\"Shape of subset training labels:\", Y_train.shape)\n",
    "\n",
    "\t#check which encoding is used\n",
    "\t#if amplitude encoding is used, then the 256 most important features are taken using PCA\n",
    "\tif encoding == 'amplitude':\n",
    "\t\tX_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "\t\tX_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\t\tpca = PCA(n_components = 256)\n",
    "\t\tX_train = pca.fit_transform(X_train_flat)\n",
    "\t\tX_test = pca.transform(X_test_flat)\n",
    "\t\treturn X_train, X_test, Y_train, Y_test\n",
    "\t#if amplitude encoding is used, then the 8 most important features are taken using PCA\n",
    "\telif encoding == 'angle':\n",
    "\t\tX_train = tf.image.resize(X_train[:], (784, 1)).numpy()\n",
    "\t\tX_test = tf.image.resize(X_test[:], (784, 1)).numpy()\n",
    "\t\tX_train, X_test = tf.squeeze(X_train), tf.squeeze(X_test)\n",
    "\n",
    "\t\tpca = PCA(8)\n",
    "\t\t\n",
    "\t\tX_train = pca.fit_transform(X_train)\n",
    "\t\tX_test = pca.transform(X_test)\n",
    "\n",
    "\t\t# Rescale for angle embedding\n",
    "\t\t\n",
    "\t\tX_train, X_test = (X_train - X_train.min()) * (np.pi / (X_train.max() - X_train.min())),\\\n",
    "\t\t\t\t\t\t  (X_test - X_test.min()) * (np.pi / (X_test.max() - X_test.min()))\n",
    "\t\treturn X_train, X_test, Y_train, Y_test\n",
    "\t\n",
    "X_train, X_test, Y_train, Y_test = data_load_and_process(encoding, num_classes, all_samples, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QCNN:\n",
    "\n",
    "1) Create the two circuits which implement the convolutional layer and the circuit which implements the pooling operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "F_1 circuit of the paper\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def CC14(params, wires):\n",
    "\t#U_CC14 r = 1\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.RY(params[i], wires=wires[i])\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.CRX(params[i + len(wires)], wires=[wires[(i - 1) % len(wires)], wires[i]])\n",
    "\t\t\n",
    "\t\n",
    "\t\n",
    "\t#U_CC14 r = -1 or 3\n",
    "\tfor i in range(0, len(wires)):\n",
    "\t\tqml.RY(params[i + 2 * len(wires)], wires=wires[i])\n",
    "\t\t\n",
    "\tif len(wires) % 3 == 0 or len(wires) == 2:\n",
    "\t\tfor i in range(len(wires) - 1, -1, -1):\n",
    "\t\t\tqml.CRX(params[i + 3 * len(wires)], wires=[wires[i], wires[(i-1) % len(wires)]])\n",
    "\t\t\t\n",
    "\telse:\n",
    "\t\tcontrol = len(wires) - 1\n",
    "\t\ttarget = (control + 3) % len(wires)\n",
    "\t\tfor i in range(len(wires) - 1, -1, -1):\n",
    "\t\t\tqml.CRX(params[i + 3 * len(wires)], wires=[wires[control], wires[target]])\n",
    "\t\t\t\n",
    "\t\t\tcontrol = target\n",
    "\t\t\ttarget = (control + 3) % len(wires)\n",
    "\n",
    "\"\"\"\n",
    "F_2 circuit of the paper\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def U_SU4(params, wires): # 15 params\n",
    "\tqml.U3(params[0], params[1], params[2], wires=wires[0])\n",
    "\tqml.U3(params[3], params[4], params[5], wires=wires[1])\n",
    "\tqml.CNOT(wires=[wires[0], wires[1]])\n",
    "\tqml.RY(params[6], wires=wires[0])\n",
    "\tqml.RZ(params[7], wires=wires[1])\n",
    "\tqml.CNOT(wires=[wires[1], wires[0]])\n",
    "\tqml.RY(params[8], wires=wires[0])\n",
    "\tqml.CNOT(wires=[wires[0], wires[1]])\n",
    "\tqml.U3(params[9], params[10], params[11], wires=wires[0])\n",
    "\tqml.U3(params[12], params[13], params[14], wires=wires[1])\n",
    "\n",
    "\"\"\"\n",
    "It implements the pooling circuit\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param wires: qubits to apply the gates\n",
    "\"\"\"\n",
    "def Pooling_ansatz(params, wires): #2 params\n",
    "\tqml.CRZ(params[0], wires=[wires[0], wires[1]])\n",
    "\tqml.PauliX(wires=wires[0])\n",
    "\tqml.CRX(params[1], wires=[wires[0], wires[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) create the structure of the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quantum Circuits for Convolutional layers\n",
    "param U: unitary that implements the convolution\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "param qubits: array that indicate to which qubit apply the convolutional layer\n",
    "\"\"\"\n",
    "def conv_layer(U, params, U_params, num_layer, qubits):\n",
    "\t\tparam0 = 0\n",
    "\t\tparam1 = len(qubits) * 2\n",
    "\t\t\n",
    "\t\t#add f_1 circuit\n",
    "\t\tfor l in range(num_layer):\n",
    "\t\t\tif len(qubits) == 8: #if it is the first layer, the F_1 circuit is \"divided\"\n",
    "\t\t\t\tfor i in range(0, len(qubits), len(qubits)//2):\n",
    "\t\t\t\t\tU(params[param0: param1], wires = qubits[i: i + len(qubits)//2])\n",
    "\t\t\telse:\n",
    "\t\t\t\tparam1 += len(qubits) * 2\n",
    "\t\t\t\tU(params[param0: param1], wires = qubits[0: len(qubits)])\n",
    "\n",
    "\t\t\t#now add the two-qubit circuit (F_2)\n",
    "\t\t\tparam0 = param1\n",
    "\t\t\tparam1 += U_params\n",
    "\t\t\tfor i in range(0, len(qubits), 2):\n",
    "\t\t\t\tU_SU4(params[param0: param1], wires = [qubits[i % len(qubits)], qubits[(i + 1) % len(qubits)]])\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(1, len(qubits), 2):\n",
    "\t\t\t\tU_SU4(params[param0: param1], wires = [qubits[i % len(qubits)], qubits[(i + 1) % len(qubits)]])\n",
    "\n",
    "\t\t\tparam0 = param1\n",
    "\t\t\tparam1 += len(qubits) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) create the structure of the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quantum Circuits for Pooling layers\n",
    "param V: unitary which implements the pooling operation\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "\"\"\"\n",
    "def pooling_layer1(V, params):\n",
    "\tV(params, wires=[7, 6]) \n",
    "\tV(params, wires=[1, 0]) \n",
    "\n",
    "def pooling_layer2(V, params):\n",
    "\tV(params, wires=[3, 2]) \n",
    "\tV(params, wires=[5, 4]) \n",
    "\n",
    "def pooling_layer3(V, params, num_classes):\n",
    "\tif num_classes == 4: #if we need only 4 classes. we trace out another qubit\n",
    "\t\tV(params, wires=[2,0])\t\t\t\t   \n",
    "\tV(params, wires=[6,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) create the structure of the QCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It implements the structure of the QCNN\n",
    "param U: unitary F_1\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param num_classes: how many classes the QNN has to predict\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "\"\"\"\n",
    "def QCNN_structure(U, params, U_params, num_classes, num_layer):\n",
    "\t#divide the number of parameters for each layer: conv layer1, pooling layer 1, conv layer 2, ...\n",
    "\t#U_params indicates the number of parameters of the F_2 circuit (the circuit applied to couple of adjacent qubit)\n",
    "\t#n_qubit * 2: is the number of parameters for the circuit F_1\n",
    "\tparam1CL = params[0: (U_params + n_qubit * 2) * num_layer]\n",
    "\tparam1PL = params[(U_params + n_qubit * 2) * num_layer: ((U_params + n_qubit * 2) * num_layer) + 2]\n",
    "\t\t\n",
    "\tparam2CL = params[((U_params + n_qubit * 2) * num_layer) + 2: ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer)]\n",
    "\tparam2PL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer): \n",
    "\t\t\t\t\t  ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2]\n",
    "\n",
    "\tparam3CL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2: \n",
    "\t\t\t\t\t  ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer)]\n",
    "\n",
    "\t#apply the circuits\n",
    "\tconv_layer(U, param1CL, U_params, num_layer, range(n_qubit))\n",
    "\tpooling_layer1(Pooling_ansatz, param1PL)\n",
    "\t\n",
    "\tconv_layer(U, param2CL, U_params, num_layer, [0, 2, 3, 4, 5, 6])\n",
    "\tpooling_layer2(Pooling_ansatz, param2PL)\n",
    "\t\n",
    "\tconv_layer(U, param3CL, U_params, num_layer, [0, 2, 4, 6])\n",
    "\n",
    "\t#if we have only 4, 6 or 8 classes, then we need another pooling layer and we need to trace out:\n",
    "\t#another qubit if we have 6 or 8 classes, because we need only 3 qubits to represent 6 or 8 classes\n",
    "\t#2 qubits if we have 4 classes, because we need only 2 qubits to represent 4 classes/states\n",
    "\t#if we have 10 classes, then we don't apply another pooling layer, because we need 4 qubits\n",
    "\tif num_classes == 4 or num_classes == 6 or num_classes == 8:\n",
    "\t\tparam3PL = params[((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer):\n",
    "\t\t\t\t\t\t ((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer) + 2]\t  \n",
    "\n",
    "\t\tpooling_layer3(Pooling_ansatz, param3PL, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the QCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define the simulator and the QCNN: encoding + VQC + measurement\n",
    "param X: sample in input\n",
    "param params: theta angle of the rotations. parameters to be trained\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: which encoding is chosen\n",
    "param num_classes: how many classes the QNN has to predict\n",
    "param num_layer: number of repetition of the convolutional layer\n",
    "return result: the probabilities of the states, which are associated to the MNIST classes\n",
    "\"\"\"\n",
    "dev = qml.device('default.qubit', wires = n_qubit)\n",
    "@qml.qnode(dev)\n",
    "def QCNN(X, params, U_params, embedding_type='amplitude', num_classes=10, num_layer = 1):\n",
    "\t# Data Embedding\n",
    "\tif embedding_type == 'amplitude':\n",
    "\t\tAmplitudeEmbedding(X, wires=range(8), normalize=True)\n",
    "\telif embedding_type == 'angle':\n",
    "\t\tAngleEmbedding(X, wires=range(8), rotation='Y')\n",
    "\t\n",
    "\t# Create the VQC\n",
    "\tQCNN_structure(CC14, params, U_params, num_classes, num_layer)\n",
    "\t\n",
    "\t#Measures the necessary qubits\n",
    "\tif num_classes == 4:\n",
    "\t\tresult = qml.probs(wires=[0, 4])\n",
    "\telif num_classes == 6:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4])\n",
    "\telif num_classes == 8:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4])\n",
    "\telse:\n",
    "\t\tresult = qml.probs(wires=[0, 2, 4, 6])\t\t\t\n",
    "\t\t\t\t\t\n",
    "\treturn result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Step:\n",
    "\n",
    "1) define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It computes the cross-entropy loss\n",
    "param labels: correct classes of the Training set\n",
    "param predictions: classes predicted by the QCNN\n",
    "param num_classes: number of classes\n",
    "return loss: average loss \n",
    "\"\"\"\n",
    "def cross_entropy(labels, predictions, num_classes):\n",
    "\tepsilon = 1e-15\n",
    "\tnum_samples = len(labels)\n",
    "\n",
    "\tnum_classes = len(predictions[0])\n",
    "\tY_true_one_hot = anp.eye(num_classes)[labels]\n",
    "\n",
    "\tloss = 0.0\n",
    "\tfor i in range(num_samples):\n",
    "\t\tpredictions[i] = anp.clip(predictions[i], epsilon, 1 - epsilon)\n",
    "\t\tloss -= anp.sum(Y_true_one_hot[i] * anp.log(predictions[i]))\t\t\n",
    "\t\n",
    "\t\n",
    "\treturn loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It executes the circuit for each image of the dataset (divided in batches)\n",
    "param calculate the loss function\n",
    "param params: the angle to be trained\n",
    "param X: batches of the training set\n",
    "param Y: batches of the training set\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: indicate the chosen encoding )\n",
    "param circ_layer: number of repetitions of the convolutional layer\n",
    "return loss: average loss\n",
    "\"\"\"\n",
    "def cost(params, X, Y, U_params, embedding_type, num_classes, circ_layer):\n",
    "\tpredictions = [QCNN(x, params, U_params, embedding_type, num_classes=num_classes, layer = circ_layer) for x in X]\n",
    "\t\n",
    "\t\n",
    "\tloss = cross_entropy(Y, predictions, num_classes)\n",
    "\t\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Execute the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss History for circuit with amplitude\n",
      "EPOCH:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marco\\miniconda3\\envs\\pl_cpu_jup\\Lib\\site-packages\\autograd\\numpy\\numpy_vjps.py:698: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  onp.add.at(A, idx, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0  cost:  2.793639592510273\n",
      "iteration:  6400  cost:  2.3454968749014977\n",
      "iteration:  12800  cost:  2.350804201469498\n",
      "iteration:  19200  cost:  2.390058187012758\n",
      "iteration:  25600  cost:  2.145615944178888\n",
      "iteration:  32000  cost:  2.1590629945116815\n",
      "iteration:  38400  cost:  2.1028156791251527\n",
      "iteration:  44800  cost:  2.269390313856259\n",
      "iteration:  51200  cost:  2.2079282103008855\n",
      "iteration:  57600  cost:  2.271460262932687\n",
      "EPOCH:  1\n",
      "iteration:  0  cost:  2.198988499641296\n",
      "iteration:  6400  cost:  2.141810916788035\n",
      "iteration:  12800  cost:  2.0999173864445906\n",
      "iteration:  19200  cost:  2.189340618483039\n",
      "iteration:  25600  cost:  2.0646491568359924\n",
      "iteration:  32000  cost:  2.132142383087968\n",
      "iteration:  38400  cost:  2.028629969450609\n",
      "iteration:  44800  cost:  2.249857211386858\n",
      "iteration:  51200  cost:  2.1752603866711544\n",
      "iteration:  57600  cost:  2.0863983708256084\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It executes the training of the QNN\n",
    "param X_train: X training set\n",
    "param Y_train: Y training set\n",
    "param U_params: number of parameters which implement a single block of the F_2 circuit\n",
    "param embedding_type: the encoding method used\n",
    "param num_classes: number of classes\n",
    "param num_layer: number of repetitions of conv layer\n",
    "param loadParams: if True the parameters are loaded from a file (used to continue a stopped training)\n",
    "param optimizer: the optimizer used\n",
    "param learning_rate: learning rate of the optimizer\n",
    "param epochs: number of epochs\n",
    "param all_samples: it all the samples are used\n",
    "param batch_size: size of the batches\n",
    "param seed: if None a random seed is used, otherwise the value in the variable\n",
    "return params: the trained parameters\n",
    "\"\"\"\n",
    "def circuit_training(X_train, Y_train, U_params, embedding_type, num_classes, num_layer, loadParams, optimizer, learning_rate, epochs, all_samples, batch_size, seed):\n",
    "\tif seed != None:\n",
    "\t\tnp.random.seed(seed)\n",
    "\t\tanp.random.seed(seed)\n",
    "\t\n",
    "\t#calculate the number of parameters\n",
    "\tif num_classes == 10:\n",
    "\t\ttotal_params =\t((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer)\n",
    "\telse: #we have to add another pooling layer at the end, so we need two parameters\n",
    "\t\ttotal_params =\t((U_params + n_qubit * 2) * num_layer) + 2 + ((U_params + (n_qubit - 2) * 4) * num_layer) + 2 + ((U_params + n_qubit * 2) * num_layer) + 2\n",
    "\t\n",
    "\t#laod the parameters\n",
    "\tif not loadParams:\n",
    "\t\tparams = np.random.randn(total_params, requires_grad=True)\n",
    "\telse:\n",
    "\t\tfileParams = open('params' + 'L' + str(num_layer) + 'LR' + str(learning_rate) + optimizer + 'C' + str(num_classes) + str(all_samples) + '.obj', 'rb')\n",
    "\n",
    "\t\tparams = pickle.load(fileParams)\n",
    "\t\tfileParams.close()\n",
    "\t\tprint(params)\n",
    "\t\n",
    "\t#choose the optimizer\n",
    "\tif optimizer == 'Adam':\n",
    "\t\topt = qml.AdamOptimizer(stepsize=learning_rate)\n",
    "\telif optimizer == 'GDO':\n",
    "\t\topt = qml.GradientDescentOptimizer(stepsize=learning_rate)\n",
    "\telse:\n",
    "\t\topt = qml.QNGOptimizer(stepsize=learning_rate)\n",
    "\t\n",
    "\t#start the training\n",
    "\tloss_history = []\n",
    "\tgrad_vals = []\n",
    "\tfor e in range(0, epochs):\n",
    "\t\tprint(\"EPOCH: \", e)\n",
    "\t\tfor b in range(0, len(X_train), batch_size):\n",
    "\t\t\tif (b + batch_size) <= len(X_train):\n",
    "\t\t\t\tX_batch = [X_train[i] for i in range(b, b + batch_size)]\n",
    "\t\t\t\tY_batch = [Y_train[i] for i in range(b, b + batch_size)]\n",
    "\t\t\telse:\n",
    "\t\t\t\tX_batch = [X_train[i] for i in range(b, len(X_train))]\n",
    "\t\t\t\tY_batch = [Y_train[i] for i in range(b, len(X_train))]\n",
    "\n",
    "\t\t\tif optimizer == 'QNGO': \n",
    "\t\t\t\tmetric_fn = lambda p: qml.metric_tensor(QCNN, approx=\"block-diag\")(X_batch, p, U_params, embedding_type, num_classes, num_layer)\n",
    "\t\t\t\tparams, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U_params, embedding_type, num_classes, num_layer),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t \tparams, metric_tensor_fn=metric_fn)\n",
    "\t\t\telse:\n",
    "\t\t\t\tparams, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, U_params, embedding_type, num_classes, num_layer),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t \tparams)\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif b % (batch_size * 100) == 0:\n",
    "\t\t\t\tprint(\"iteration: \", b, \" cost: \", cost_new)\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\tloss_history.append(cost_new)\n",
    "\t\t\t\tgradient_fn = qml.grad(cost)\n",
    "\t\t\t\tgradients = gradient_fn(params, X_batch, Y_batch, U, U_params, embedding_type, cost_fn, num_classes, num_layer)\n",
    "\t\t\t\tgrad_vals.append(gradients[-1])\n",
    "\t\t\t\tprint(gradients)\n",
    "\t\t\t\tprint(\"var \", np.var(grad_vals))\n",
    "\t\t\t\tprint(\"mean grad: \", np.mean(grad_vals))\n",
    "\t\t\t\t\"\"\"\n",
    "\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t#save the novel parameters at the end of each epoch\t\n",
    "\t\tfileParams = open('params' + 'L' + str(num_layer) + 'LR' + str(learning_rate) + optimizer + 'C' + str(num_classes) + str(all_samples) + '.obj', 'wb')\n",
    "\n",
    "\n",
    "\t\tpickle.dump(params, fileParams)\n",
    "\t\tfileParams.close()\n",
    "\treturn params\n",
    "\n",
    "print(\"Loss History for circuit with \" + encoding)\n",
    "trained_params = circuit_training(X_train, Y_train, U_params, encoding, num_classes, num_layer, load_params,\n",
    "\t\t\t\t\t\t\t\topt, lr, epochs, all_samples, batch_size, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infere on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5674\n",
      "[[[8613  407]\n",
      "  [ 447  533]]\n",
      "\n",
      " [[8064  801]\n",
      "  [  89 1046]]\n",
      "\n",
      " [[8465  503]\n",
      "  [ 479  553]]\n",
      "\n",
      " [[8758  232]\n",
      "  [ 625  385]]\n",
      "\n",
      " [[8284  734]\n",
      "  [ 445  537]]\n",
      "\n",
      " [[8886  222]\n",
      "  [ 659  233]]\n",
      "\n",
      " [[8660  382]\n",
      "  [ 234  724]]\n",
      "\n",
      " [[8747  225]\n",
      "  [ 251  777]]\n",
      "\n",
      " [[8736  290]\n",
      "  [ 490  484]]\n",
      "\n",
      " [[8461  530]\n",
      "  [ 607  402]]]\n",
      "precision 0: 0.5670212765957446\n",
      "recall 0: 0.5438775510204081\n",
      "f1 0: 0.5552083333333332\n",
      "precision 1: 0.5663237682728749\n",
      "recall 1: 0.9215859030837005\n",
      "f1 1: 0.7015425888665324\n",
      "precision 2: 0.5236742424242424\n",
      "recall 2: 0.5358527131782945\n",
      "f1 2: 0.5296934865900381\n",
      "precision 3: 0.6239870340356564\n",
      "recall 3: 0.3811881188118812\n",
      "f1 3: 0.47326367547633663\n",
      "precision 4: 0.4225019669551534\n",
      "recall 4: 0.5468431771894093\n",
      "f1 4: 0.4766977363515312\n",
      "precision 5: 0.512087912087912\n",
      "recall 5: 0.26121076233183854\n",
      "f1 5: 0.345953971789161\n",
      "precision 6: 0.6546112115732369\n",
      "recall 6: 0.755741127348643\n",
      "f1 6: 0.701550387596899\n",
      "precision 7: 0.7754491017964071\n",
      "recall 7: 0.7558365758754864\n",
      "f1 7: 0.7655172413793102\n",
      "precision 8: 0.6253229974160207\n",
      "recall 8: 0.49691991786447637\n",
      "f1 8: 0.5537757437070937\n",
      "precision 9: 0.4313304721030043\n",
      "recall 9: 0.398414271555996\n",
      "f1 9: 0.4142194744976815\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It computes the accuracy on the test set\n",
    "param predictions: classes predicted\n",
    "param labels: true classes\n",
    "param num_classes: number of classes\n",
    "return accuracy: accuracy \n",
    "\"\"\"\n",
    "def accuracy_multi(predictions, labels, num_classes):\n",
    "\tcorrect_predictions = 0\n",
    "\n",
    "\t\n",
    "\tfor l, p in zip(labels, predictions):\n",
    "\t\tp2 = []\n",
    "\t\tfor i in range(0, num_classes):\n",
    "\t\t\tp2.append(p[i])\n",
    "\t\tpredicted_class = np.argmax(p2)\t# Find the index of the predicted class with highest probability\n",
    "\t\tif predicted_class == l:\n",
    "\t\t\tcorrect_predictions += 1\n",
    "\n",
    "\taccuracy = correct_predictions / len(labels)\n",
    "\treturn accuracy\n",
    "\n",
    "\"\"\"\n",
    "It computes the precision, recall, F1 score and Confusion Matrix on the test set\n",
    "param predictions: classes predicted\n",
    "param labels: true classes\n",
    "param num_classes: number of classes\n",
    "return accuracy: accuracy \n",
    "\"\"\"\n",
    "def accuracy_test_multiclass(predictions, label, num_classes):\n",
    "\t#confusion matrix\n",
    "\t\n",
    "\tpreds_np = np.array(predictions)\n",
    "\tpreds = np.argmax(preds_np[:, :num_classes], axis = 1)\n",
    "\t\n",
    "\tconf_mat = multilabel_confusion_matrix(label, preds, labels = list(range(num_classes)))\n",
    "\tprint(conf_mat)\n",
    "\tprecision = []\n",
    "\trecall = []\n",
    "\tf1 = []\n",
    "\ti = 0\n",
    "\tfor c in conf_mat:\n",
    "\t\tprecision.append(c[1][1] / (c[1][1] + c[0][1]))\n",
    "\t\trecall.append(c[1][1] / (c[1][1] + c[1][0]))\n",
    "\t\tf1.append(2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + np.finfo(float).eps))\n",
    "\t\t\n",
    "\t\tprint(\"precision \" + str(i) + \": \" + str(precision[i])) \n",
    "\t\tprint(\"recall \" + str(i) + \": \" + str(recall[i])) \n",
    "\t\tprint(\"f1 \" + str(i) + \": \" + str(f1[i])) \n",
    "\t\ti += 1\n",
    "\n",
    "predictions = []\n",
    "\t\t\t\t\n",
    "for x in X_test:\t\n",
    "\tpredictions.append(QCNN(x, trained_params, U_params, encoding, num_classes, num_layer))\n",
    "\t\t\t\t\t\t\t\n",
    "accuracy = accuracy_multi(predictions, Y_test, num_classes)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "accuracy_test_multiclass(predictions, Y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
